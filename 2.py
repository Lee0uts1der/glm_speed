import time
from typing import List, Dict
import pandas as pd
from zhipuai import ZhipuAI

# åˆå§‹åŒ– clientï¼ˆè¯·å¡«å†™ä½ çš„ API keyï¼‰
client = ZhipuAI(api_key="3c87b4c80506418a8882bef064ec2199.BaNPFlZcQHfJOFyg")

# æ ¸å¿ƒæµ‹é€Ÿå‡½æ•°ï¼ˆä¸ä¾èµ– stream_optionsï¼‰
def measure_glm_speed(client, model: str, messages: List[Dict[str, str]]) -> Dict:
    start_time = time.time()
    first_token_time = None
    content_all = ""

    # å‘èµ·æµå¼è¯·æ±‚ï¼ˆä¸åŒ…å« stream_optionsï¼‰
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True
    )

    for chunk in response:
        if not chunk.choices:
            continue
        delta = chunk.choices[0].delta
        reasoning_piece = getattr(delta, 'reasoning_content', "")
        content_piece = getattr(delta, 'content', "")

        if first_token_time is None and (reasoning_piece or content_piece):
            first_token_time = time.time() - start_time

        print(reasoning_piece + content_piece, end="")
        content_all += reasoning_piece + content_piece

    end_time = time.time()

    # ç²—ç•¥ä¼°ç®— token æ•°ï¼ˆ1 token â‰ˆ 1.5 ä¸ªå­—ç¬¦ï¼‰
    estimated_tokens = int(len(content_all) / 1.5)
    generation_time = end_time - (start_time + first_token_time)
    avg_token_speed = estimated_tokens / generation_time if generation_time > 0 else 0

    return {
        "é¦– token å»¶è¿Ÿï¼ˆç§’ï¼‰": round(first_token_time, 3),
        "ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰": round(generation_time, 3),
        "ä¼°ç®— tokens æ•°": estimated_tokens,
        "æ€»è€—æ—¶ï¼ˆç§’ï¼‰": round(end_time - start_time, 3),
        "å¹³å‡ç”Ÿæˆé€Ÿåº¦ (token/s)": round(avg_token_speed, 2)
    }

# å¤šè½®æµ‹é€Ÿå‡½æ•°
def batch_measure_speed(client, model: str, prompts: List[str]) -> pd.DataFrame:
    results = []

    for idx, prompt in enumerate(prompts):
        print(f"\nğŸ§ª æ­£åœ¨æµ‹é‡ç¬¬ {idx + 1} ä¸ªé—®é¢˜ï¼š{prompt}\n")
        messages = [{"role": "user", "content": prompt}]
        result = measure_glm_speed(client, model, messages)
        result["é—®é¢˜ç¼–å·"] = f"Q{idx + 1}"
        result["é—®é¢˜å†…å®¹"] = prompt
        results.append(result)

    df = pd.DataFrame(results)
    cols = ["é—®é¢˜ç¼–å·", "é—®é¢˜å†…å®¹", "é¦– token å»¶è¿Ÿï¼ˆç§’ï¼‰", "ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰",
            "ä¼°ç®— tokens æ•°", "æ€»è€—æ—¶ï¼ˆç§’ï¼‰", "å¹³å‡ç”Ÿæˆé€Ÿåº¦ (token/s)"]
    df = df[cols]
    return df

# ç¨‹åºå…¥å£ï¼ˆä¸»å‡½æ•°ï¼‰
if __name__ == "__main__":
    prompts = [
        "è¢‹å­é‡Œæœ‰5ä¸ªçº¢çƒå’Œ3ä¸ªè“çƒï¼ŒéšæœºæŠ½ä¸¤ä¸ªçƒï¼Œè‡³å°‘ä¸€ä¸ªçº¢çƒçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ",
        "è¯·å†™ä¸€ç¯‡è®®è®ºæ–‡ï¼Œè®ºè¿°å­¦ä¹ ä½¿ç”¨AIçš„é‡è¦æ€§ã€‚",
        "9.11å’Œ9.9ä¸¤ä¸ªæ•°å­—ï¼Œè°æ›´å¤§ï¼Ÿ"
    ]

    df = batch_measure_speed(client, model="glm-z1-airx", prompts=prompts)
    print("\nğŸ“Š æµ‹è¯•ç»“æœå¦‚ä¸‹ï¼š\n")
    print(df.to_markdown(index=False))

    df.to_csv("glm_speed_results.csv", index=False)
    print("\nâœ… å·²ä¿å­˜ç»“æœä¸º glm_speed_results.csv")
